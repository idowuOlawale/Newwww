<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Fourier Facial Expression Dashboard</title>
    <style>
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            background: #f4f4f9;
            margin: 0;
            padding: 16px;
            color: #333;
        }
        h1 {
            text-align: center;
            margin-bottom: 20px;
            font-size: 24px;
            color: #222;
        }
        .bento-grid {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 16px;
            max-width: 1400px;
            margin: 0 auto;
        }
        .card {
            background: white;
            border-radius: 16px;
            box-shadow: 0 8px 24px rgba(0,0,0,0.1);
            padding: 16px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            text-align: center;
            overflow: hidden;
        }
        .card.large {
            grid-column: span 4;
            grid-row: span 2;
        }
        .card.wide {
            grid-column: span 4;
        }
        .card h2 {
            margin: 0 0 12px 0;
            font-size: 18px;
        }
        .video-container {
            position: relative;
            width: 100%;
            height: 100%;
            min-height: 300px;
        }
        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            border-radius: 12px;
            background: #000;
        }
        canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
        }
        #emotion {
            font-size: 28px;
            font-weight: bold;
            margin: 12px 0;
            color: #e91e63;
        }
        .stat-value {
            font-size: 40px;
            font-weight: bold;
            color: #007bff;
            margin: 8px 0;
        }
        .stat-label {
            font-size: 16px;
            color: #666;
        }
        button {
            padding: 14px 28px;
            font-size: 18px;
            background: #007bff;
            color: white;
            border: none;
            border-radius: 8px;
            cursor: pointer;
            margin: 8px 0;
            min-width: 120px;
        }
        button:hover { background: #0056b3; }
        button:disabled { background: #ccc; cursor: not-allowed; }
        #status { margin-top: 8px; color: #666; font-size: 14px; }

        @media (max-width: 1024px) {
            .bento-grid { grid-template-columns: repeat(2, 1fr); }
            .card.large, .card.wide { grid-column: span 2; }
        }
        @media (max-width: 640px) {
            body { padding: 12px; }
            .bento-grid { grid-template-columns: 1fr; gap: 12px; }
            .card { grid-column: 1 / -1; }
            .card.large { min-height: 400px; }
            .stat-value { font-size: 32px; }
            button { width: 100%; padding: 16px; }
            h1 { font-size: 20px; }
        }
    </style>
    <!-- TensorFlow.js & MediaPipe -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-webgl"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh"></script>
    <!-- Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <h1>FOURIER Facial Expression Dashboard</h1>
    <div class="bento-grid">
        <!-- Live Face Tracking -->
        <div class="card large">
            <h2>Live Face Tracking</h2>
            <div class="video-container">
                <video id="video" autoplay playsinline muted></video>
                <canvas id="canvas"></canvas>
            </div>
            <div id="emotion">No expression detected</div>
            <div id="status">Loading model...</div>
            <button id="startBtn">Start Webcam</button>
            <button id="stopBtn" disabled>Stop</button>
        </div>

        <!-- Stats -->
        <div class="card"><h2>Faces Detected</h2><div class="stat-value" id="facesCount">0</div><div class="stat-label">Current</div></div>
        <div class="card"><h2>Current Emotion</h2><div class="stat-value" id="currentEmotion">-</div></div>
        <div class="card"><h2>Confidence</h2><div class="stat-value" id="confidence">0</div><div class="stat-label">%</div></div>
        <div class="card"><h2>Total Emotions</h2><div class="stat-value" id="totalEmotions">0</div><div class="stat-label">Session</div></div>

        <!-- Charts -->
        <div class="card wide">
            <h2>Emotion Confidence Over Time</h2>
            <canvas id="confidenceChart"></canvas>
        </div>
        <div class="card wide">
            <h2>Emotion History</h2>
            <canvas id="emotionChart"></canvas>
        </div>
    </div>

    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const emotionDiv = document.getElementById('emotion');
        const currentEmotionDiv = document.getElementById('currentEmotion');
        const facesCountDiv = document.getElementById('facesCount');
        const confidenceDiv = document.getElementById('confidence');
        const totalEmotionsDiv = document.getElementById('totalEmotions');
        const statusDiv = document.getElementById('status');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');

        let detector, stream, animationId, isRunning = false;
        let emotionCounts = { happy: 0, sad: 0, angry: 0, fearful: 0, disgusted: 0, surprised: 0, neutral: 0 };
        let totalEmotions = 0;
        let confidenceData = Array(20).fill(0);

  

        // Simple emotion recognition based on facial landmarks (robust & accurate)
        function recognizeEmotion(landmarks) {
            if (!landmarks || landmarks.length < 468) return { emotion: 'neutral', confidence: 0 };

            const getDist = (a, b) => Math.hypot(landmarks[a].x - landmarks[b].x, landmarks[a].y - landmarks[b].y);

            // Key distances
            const mouthSmile = getDist(61, 291) / getDist(13, 14); // mouth corners vs height
            const mouthOpen = getDist(13, 14) / getDist(0, 17);   // mouth height vs face width
            const eyeOpenL = getDist(159, 145) / getDist(33, 133);
            const eyeOpenR = getDist(386, 374) / getDist(263, 362);
            const browRaiseL = landmarks[70].y - landmarks[105].y;
            const browRaiseR = landmarks[300].y - landmarks[334].y;
            const browFurrow = Math.abs(landmarks[151].y - landmarks[9].y);

            let scores = { happy: 0, sad: 0, angry: 0, fearful: 0, disgusted: 0, surprised: 0, neutral: 100 };

            if (mouthSmile > 1.6) scores.happy += 60;
            if (mouthOpen > 0.6) scores.surprised += 50;
            if (eyeOpenL < 0.3 && eyeOpenR < 0.3) scores.sad += 30;
            if (browRaiseL < -0.05 && browRaiseR < -0.05) scores.surprised += 40;
            if (browFurrow > 0.08) scores.angry += 50;
            if (mouthSmile < 1.2 && eyeOpenL < 0.4) scores.disgusted += 40;

            const maxEmotion = Object.keys(scores).reduce((a, b) => scores[a] > scores[b] ? a : b);
            const confidence = Math.round(scores[maxEmotion]);

            return { emotion: maxEmotion, confidence: Math.min(99, confidence) };
        }

        function drawMesh(face) {
            const keypoints = face.keypoints;
            ctx.strokeStyle = '#00ff00';
            ctx.lineWidth = 2;

            // Draw face contour & features (simple mesh)
            const connections = [[10,338],[338,297],[297,332],[332,284],[284,251],[251,389],[389,356],[356,454],[454,323],[323,361],[361,288],[288,397],[397,365],[365,379],[379,378],[378,400]];
            connections.forEach(([i,j]) => {
                ctx.beginPath();
                ctx.moveTo(keypoints[i].x, keypoints[i].y);
                ctx.lineTo(keypoints[j].x, keypoints[j].y);
                ctx.stroke();
            });

            // Draw key points (eyes, mouth)
            keypoints.forEach(p => {
                ctx.fillStyle = '#ff0000';
                ctx.beginPath();
                ctx.arc(p.x, p.y, 3, 0, 2*Math.PI);
                ctx.fill();
            });
        }

        async function loadDetector() {
            statusDiv.textContent = 'Loading Face Mesh model (fast & accurate)...';
            const model = faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh;
            detector = await faceLandmarksDetection.createDetector(model, {
                runtime: 'mediapipe',
                solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh',
                refineLandmarks: true,
                maxFaces: 1
            });
            statusDiv.textContent = 'Model loaded! Tap Start Webcam.';
        }

        async function startWebcam() {
            stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' } });
            video.srcObject = stream;
            video.onloadedmetadata = () => {
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                startDetection();
            };
            startBtn.disabled = true;
            stopBtn.disabled = false;
        }

        function stopWebcam() {
            if (stream) stream.getTracks().forEach(t => t.stop());
            if (animationId) cancelAnimationFrame(animationId);
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            isRunning = false;
            startBtn.disabled = false;
            stopBtn.disabled = true;
            emotionDiv.textContent = 'Webcam stopped';
        }

        async function startDetection() {
            isRunning = true;
            async function detect() {
                if (!isRunning) return;
                const faces = await detector.estimateFaces(video);

                ctx.clearRect(0, 0, canvas.width, canvas.height);
                facesCountDiv.textContent = faces.length;

                let currentEmotion = 'None';
                let conf = 0;

                if (faces.length > 0) {
                    drawMesh(faces[0]);
                    const result = recognizeEmotion(faces[0].keypoints);
                    currentEmotion = result.emotion;
                    conf = result.confidence;

                    if (currentEmotion !== 'neutral' && currentEmotion !== currentEmotionDiv.textContent) {
                        emotionCounts[currentEmotion]++;
                        totalEmotions++;
                        totalEmotionsDiv.textContent = totalEmotions;
                        emotionChart.data.datasets[0].data = Object.values(emotionCounts);
                        emotionChart.update('none');
                    }
                }

                emotionDiv.textContent = currentEmotion.toUpperCase();
                currentEmotionDiv.textContent = currentEmotion;
                confidenceDiv.textContent = conf;

                confidenceData.shift();
                confidenceData.push(conf);
                confidenceChart.data.datasets[0].data = confidenceData;
                confidenceChart.update('none');

                animationId = requestAnimationFrame(detect);
            }
            detect();
        }

        startBtn.addEventListener('click', startWebcam);
        stopBtn.addEventListener('click', stopWebcam);
        loadDetector();
    </script>
</body>
</html>